{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\yche465\\\\Desktop\\\\CS570\\\\Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys, os, shutil\n",
    "import pandas as pd\n",
    "import collections as cl\n",
    "import itertools as it\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import copy as cp\n",
    "\n",
    "#Change directory\n",
    "os.chdir(\"C:/Users/yche465/Desktop/CS570/Project\")\n",
    "os.getcwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw data to PDC time series (for two-stage clustering analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import raw dataset\n",
    "data=pd.read_sas(\"prep.sas7bdat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude patients that did not have 2-year follow up period\n",
    "startdate=data.groupby(\"pid\").agg('min').drop(columns=['Rx_end'])\n",
    "startdate=startdate.rename(columns={'Rx_start':'Initiation'})\n",
    "data2=data.merge(startdate, on=\"pid\",how='left')\n",
    "exdate=max(data['Rx_end'])-dt.timedelta(days=365*2)\n",
    "data3=data2[data2['Initiation']<exdate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWID=pd.DataFrame({\"pid\":np.unique(data3['pid']),\"PID\":np.array(range(len(np.unique(data3['pid']))))+1})\n",
    "data3_=data3.merge(NEWID, on='pid', how='left')\n",
    "data3=data3_.drop(columns='pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#de-identify the date\n",
    "a=data3[\"Rx_start\"] - data3[\"Initiation\"] \n",
    "data3['Rx_start_days']=a.dt.days\n",
    "a=data3[\"Rx_end\"] - data3[\"Initiation\"] \n",
    "data3['Rx_end_days']=a.dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute length of follow up\n",
    "enddate=pd.DataFrame(data3.groupby(\"PID\").agg('max')['Rx_end']).rename(columns={'Rx_end':'LFU'})\n",
    "data4=data3.merge(enddate, on=\"PID\",how='left')\n",
    "FUT=data4.groupby('PID').agg('max')[['Initiation','LFU']]\n",
    "data4['obs']=np.array(range(len(data4)))\n",
    "dur=FUT['LFU']-FUT['Initiation']\n",
    "FUT['FUT']=dur.dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the first observations of any users with <14 PrEP duration\n",
    "length=1\n",
    "while length>0:\n",
    "    firstobs=data4.groupby(\"PID\").first()\n",
    "    searchnext=firstobs[np.array(firstobs['Rx_end_days']-firstobs['Rx_start_days']+1<14)]\n",
    "    obs_todelete=pd.Index(np.array(searchnext['obs']))\n",
    "    data4.drop(obs_todelete,inplace=True)\n",
    "    length=len(obs_todelete)\n",
    "    \n",
    "# re-calibrate the initiation date\n",
    "startdate=data4.groupby(\"PID\").agg('min')\n",
    "startdate=startdate.rename(columns={'Rx_start_days':'Initiation2_days', 'Rx_start':'Initiation2'})\n",
    "startdate2=startdate[['Initiation2','Initiation2_days']]\n",
    "data5=data4.merge(startdate2, on=\"PID\",how='left')\n",
    "data5['Rx_start_days2']=data5['Rx_start_days']-data5['Initiation2_days']\n",
    "data5['Rx_end_days2']=data5['Rx_end_days']-data5['Initiation2_days']\n",
    "#Include only the participants whose updated initiation date is earlier than the hypothetical start date of the 2-yr follow up period  \n",
    "data6=data5[data5['Initiation2']<exdate]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create final data frame\n",
    "NEWID=pd.DataFrame({\"PID\":np.unique(data6['PID']),\"PID2\":np.array(range(len(np.unique(data6['PID']))))+1})\n",
    "data6_=data6.merge(NEWID, on='PID', how='left')\n",
    "data6=data6_.drop(columns='PID')\n",
    "fdata=data6[['obs','PID2','Rx_start_days2','Rx_end_days2']].rename(columns={'Rx_start_days2':'Rx_start_days','Rx_end_days2':'Rx_end_days','PID2':'PID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make PDC matrix (for two-stage clustering analysis)\n",
    "id_pool=np.unique(fdata[\"PID\"])\n",
    "medcover=np.zeros((len(id_pool),365*2))\n",
    "for k in range(len(id_pool)):\n",
    "    Rx_start=np.array(fdata.loc[fdata[\"PID\"]==id_pool[k],\"Rx_start_days\"])\n",
    "    Rx_end=np.array(fdata.loc[fdata[\"PID\"]==id_pool[k],\"Rx_end_days\"])\n",
    "    \n",
    "    for j in range(len(Rx_start)):\n",
    "        medcover[k][Rx_start[j]:Rx_end[j]+1]=1\n",
    "        \n",
    "start=np.array(range(0,103*7,7))\n",
    "end=start+14\n",
    "PDC=np.zeros((len(id_pool),103))\n",
    "\n",
    "for k in range(len(id_pool)):    \n",
    "    for i in range(len(start)):\n",
    "        PDC[k][i]=sum(medcover[k][start[i]:end[i]])/14\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export PDC time series data\n",
    "# f= open(\"PDC.txt\",\"w+\")\n",
    "# for i in range(len(PDC)):\n",
    "#     for j in range(103):\n",
    "#         if j == 102:\n",
    "#             f.write(\"%s\\n\" % (PDC[i][j]))\n",
    "#         else:\n",
    "#             f.write(\"%s,\" % (PDC[i][j]))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create time series dataset for numbers of days under medication (for GBTM analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make matrix of number of days under meds (for GBTM analysis )\n",
    "start2=np.arange(0,7*105,14)\n",
    "DrugDay=np.zeros((len(id_pool),len(start2)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(id_pool)):    \n",
    "    for i in range(1,len(start2)):\n",
    "        DrugDay[k][i-1]=sum(medcover[k][start2[i-1]:start2[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export number of days under meds time series data\n",
    "# f= open(\"DrugDay.txt\",\"w+\")\n",
    "# for i in range(len(DrugDay)):\n",
    "#     for j in range(52):\n",
    "#         if j == 51:\n",
    "#             f.write(\"%s\\n\" % (DrugDay[i][j]))\n",
    "#         else:\n",
    "#             f.write(\"%s,\" % (DrugDay[i][j]))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Stage-I Clustering:\n",
    "\n",
    "Create a data matrix object with the following three fields: 1) the total duration spent in PrEP cessation; 2) the number of the number of PrEP interval; 3) Time of first PrEP cessation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PDC dataset\n",
    "PDC = np.loadtxt('PDC.txt', usecols=range(103), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) the number of PrEP cessations (i.e., interval where PDCs were <0.57)\n",
    "Cess=PDC<4/7\n",
    "Cessation=[]\n",
    "for k in range(len(Cess)):\n",
    "    t=0\n",
    "    for i in range(102):\n",
    "        if Cess[k][i]==False and Cess[k][i+1]==True:\n",
    "            t+=1\n",
    "    Cessation.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_WEEK=[]\n",
    "PSTART_WEEK=[]\n",
    "END_WEEK=[]\n",
    "PEND_WEEK=[]\n",
    "for k in range(len(Cess)):\n",
    "    start_week=[]\n",
    "    pstart_week=[0]\n",
    "    end_week=[]\n",
    "    pend_week=[]\n",
    "    for i in range(1,103):\n",
    "        if Cess[k][i]==True and Cess[k][i-1]==False:\n",
    "            start_week.append(i)\n",
    "            pend_week.append(i)\n",
    "            \n",
    "        elif Cess[k][i]==False and Cess[k][i-1]==True:\n",
    "            end_week.append(i)\n",
    "            pstart_week.append(i)\n",
    "            \n",
    "    START_WEEK.append(start_week)\n",
    "    PSTART_WEEK.append(pstart_week)\n",
    "    END_WEEK.append(end_week)\n",
    "    PEND_WEEK.append(pend_week)\n",
    "\n",
    "\n",
    "START_WEEK_TOT=cp.deepcopy(START_WEEK)\n",
    "END_WEEK_TOT=cp.deepcopy(END_WEEK)\n",
    "Pr_START_WEEK=cp.deepcopy(PSTART_WEEK)\n",
    "Pr_END_WEEK=cp.deepcopy(PEND_WEEK)\n",
    "\n",
    "for i in range(len(Cess)):\n",
    "    if len(Pr_START_WEEK[i])>len(Pr_END_WEEK[i]):\n",
    "        Pr_END_WEEK[i].append(103)\n",
    "\n",
    "# make time series of start and end week\n",
    "for i in range(len(Cess)):   \n",
    "    if len(START_WEEK_TOT[i])>len(END_WEEK_TOT[i]):\n",
    "        END_WEEK_TOT[i].append(103)\n",
    "    elif len(START_WEEK_TOT[i])<len(END_WEEK_TOT[i]):\n",
    "        START_WEEK_TOT[i].insert(0,0)\n",
    "    elif len(START_WEEK[i])==len(END_WEEK[i]) and len(START_WEEK[i])>0 and START_WEEK[i][0]>END_WEEK[i][0]:\n",
    "        START_WEEK_TOT[i].insert(0,0)\n",
    "        END_WEEK_TOT[i].append(103)\n",
    "    elif len(START_WEEK[i])==0 and len(START_WEEK[i])==0 and sum(Cess[i])>0:\n",
    "        START_WEEK_TOT[i].insert(0,0)\n",
    "        END_WEEK_TOT[i].append(103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list object: the total duration spent in PrEP cessation\n",
    "Total_Cess_Dur=np.array([sum(Cess[i]) for i in range(len(Cess))])\n",
    "#Create list object: the number of the number of PrEP interval\n",
    "Num_PrEP=np.array([len(Pr_START_WEEK[i]) for i in range(len(Cess))])\n",
    "#Create list object: Time of first cessation\n",
    "Cess_1st=[]\n",
    "for i in range(len(START_WEEK_TOT)):\n",
    "    if len(START_WEEK_TOT[i])==0:\n",
    "        Cess_1st.append(104)\n",
    "    else:\n",
    "        Cess_1st.append(START_WEEK_TOT[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as prep\n",
    "#Sample dataset\n",
    "Data=np.dstack((Total_Cess_Dur,Num_PrEP,Cess_1st))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle_out=open(\"UnSTD_DATA\",\"wb\")\n",
    "# pickle.dump(Data,pickle_out)\n",
    "# pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Stage-II Clustering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list objects for PDC time series samples (sample size: 1%, 5%, 10%, 15%) grouped by stage-I cluster memberships and a matrix object for pair-wise  DTW distance among the corresponding samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import dtw, dtw_path\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "#Import the PDC time series data matrix grouped by stage-I cluster membership (extracted in the jupyter notebook entitled \"Stage I Clustering\")\n",
    "import pickle\n",
    "pickle_in=open(\"PDC_S1\",\"rb\")\n",
    "loadobj=pickle.load(pickle_in)\n",
    "PDC_S1=loadobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample size = 1%\n",
    "np.random.seed(0)\n",
    "samplesize=[np.int(0.01*len(PDC_S1[i])) for i in range(len(PDC_S1))]\n",
    "PDC_S1_sample=[]\n",
    "for i in range(len(PDC_S1)):\n",
    "    selected=np.random.choice(len(PDC_S1[i]),samplesize[i])\n",
    "    s= PDC_S1[i][selected]\n",
    "    PDC_S1_sample.append(s)\n",
    "    \n",
    "DISTM=[]\n",
    "for k in range(len(PDC_S1_sample)):\n",
    "    num=len(PDC_S1_sample[k])\n",
    "    dist_matrix=np.zeros((num,num))\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            dist_matrix[i,j]=dtw(PDC_S1_sample[k][i],PDC_S1_sample[k][j],global_constraint=\"sakoe_chiba\", sakoe_chiba_radius=10)\n",
    "    DISTM.append(dist_matrix)  \n",
    "\n",
    "DISTM_1=cp.deepcopy(DISTM)\n",
    "PDC_S1_sample1=cp.deepcopy(PDC_S1_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample size = 5%\n",
    "np.random.seed(0)\n",
    "samplesize=[np.int(0.05*len(PDC_S1[i])) for i in range(len(PDC_S1))]\n",
    "PDC_S1_sample=[]\n",
    "for i in range(len(PDC_S1)):\n",
    "    selected=np.random.choice(len(PDC_S1[i]),samplesize[i])\n",
    "    s= PDC_S1[i][selected]\n",
    "    PDC_S1_sample.append(s)\n",
    "    \n",
    "DISTM=[]\n",
    "for k in range(len(PDC_S1_sample)):\n",
    "    num=len(PDC_S1_sample[k])\n",
    "    dist_matrix=np.zeros((num,num))\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            dist_matrix[i,j]=dtw(PDC_S1_sample[k][i],PDC_S1_sample[k][j],global_constraint=\"sakoe_chiba\", sakoe_chiba_radius=10)\n",
    "    DISTM.append(dist_matrix)  \n",
    "\n",
    "DISTM_5=cp.deepcopy(DISTM)\n",
    "PDC_S1_sample5=cp.deepcopy(PDC_S1_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample size = 10%\n",
    "np.random.seed(0)\n",
    "samplesize=[np.int(0.10*len(PDC_S1[i])) for i in range(len(PDC_S1))]\n",
    "PDC_S1_sample=[]\n",
    "for i in range(len(PDC_S1)):\n",
    "    selected=np.random.choice(len(PDC_S1[i]),samplesize[i])\n",
    "    s= PDC_S1[i][selected]\n",
    "    PDC_S1_sample.append(s)\n",
    "    \n",
    "DISTM=[]\n",
    "for k in range(len(PDC_S1_sample)):\n",
    "    num=len(PDC_S1_sample[k])\n",
    "    dist_matrix=np.zeros((num,num))\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            dist_matrix[i,j]=dtw(PDC_S1_sample[k][i],PDC_S1_sample[k][j],global_constraint=\"sakoe_chiba\", sakoe_chiba_radius=10)\n",
    "    DISTM.append(dist_matrix)  \n",
    "\n",
    "DISTM_10=cp.deepcopy(DISTM)\n",
    "PDC_S1_sample10=cp.deepcopy(PDC_S1_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample size = 15%\n",
    "np.random.seed(0)\n",
    "samplesize=[np.int(0.15*len(PDC_S1[i])) for i in range(len(PDC_S1))]\n",
    "PDC_S1_sample=[]\n",
    "for i in range(len(PDC_S1)):\n",
    "    selected=np.random.choice(len(PDC_S1[i]),samplesize[i])\n",
    "    s= PDC_S1[i][selected]\n",
    "    PDC_S1_sample.append(s)\n",
    "\n",
    "DISTM=[]\n",
    "for k in range(len(PDC_S1_sample)):\n",
    "    num=len(PDC_S1_sample[k])\n",
    "    dist_matrix=np.zeros((num,num))\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            dist_matrix[i,j]=dtw(PDC_S1_sample[k][i],PDC_S1_sample[k][j],global_constraint=\"sakoe_chiba\", sakoe_chiba_radius=10)\n",
    "    DISTM.append(dist_matrix)  \n",
    "    \n",
    "DISTM_15=cp.deepcopy(DISTM)\n",
    "PDC_S1_sample15=cp.deepcopy(PDC_S1_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_obj={\"DISTM_15\":DISTM_15,\"DISTM_10\":DISTM_10,\"DISTM_5\": DISTM_5, \"DISTM_1\":DISTM_1,\n",
    "           \"PDC_S1_sample1\":PDC_S1_sample1,\"PDC_S1_sample5\":PDC_S1_sample5,\"PDC_S1_sample10\":PDC_S1_sample10, \"PDC_S1_sample15\":PDC_S1_sample15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the PDC samples and precomputed distance matrix as pickle object for later use in stage-II clustering\n",
    "import pickle\n",
    "pickle_out=open(\"sPDC_DISTM\",\"wb\")\n",
    "pickle.dump(store_obj,pickle_out)\n",
    "pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
